{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_pcanet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starkdg/pyConvnetPhash/blob/master/train_pcanet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jjcK55t-TFMq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "model_dir = \"/gdrive/My Drive/models\"\n",
        "module_inception_url = \"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\"\n",
        "module_mobilenetv2_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2\"\n",
        "\n",
        "module = hub.Module(module_mobilenetv2_url)\n",
        "target_height, target_width = hub.get_expected_image_size(module)\n",
        "\n",
        "n_inputs = 1792\n",
        "n_hidden = 256\n",
        "\n",
        "# location of testing .tfrecord files\n",
        "training_files_dir = \"/gdrive/My Drive/imageset/train\"\n",
        "validation_files_dir = \"/gdrive/My Drive/imageset/validation\"\n",
        "testing_files_dir = \"/gdrive/My Drive/imageset/test\"\n",
        "\n",
        "batch_size = 10\n",
        "epochs = 15\n",
        "steps = 2000\n",
        "learning_rate = 0.0001\n",
        "norm_constant = 5.0                # input normalization constant (divide input features by)\n",
        "transfer_function = tf.identity    # activation on hidden layer - other values: tf.identity, tf.nn.sigmoid, tf.nn.relu\n",
        "model_tag = \"linear\"               # location and file of exported frozen model file: \"linear\", \"sigmoid\", \"relue\"\n",
        "\n",
        "frozen_model = \"/gdrive/My Drive/models/pca_autoencoder/mobilenetv2_pca_autoenc_{0}to{1}_frozen_model-{2}.pb\".format(n_inputs, n_hidden, model_tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OzKxh7rdTjgJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = dict()\n",
        "with tf.variable_scope(\"weights\", reuse=tf.AUTO_REUSE):\n",
        "  weights['w1'] = tf.get_variable('w1', shape=[n_inputs, n_hidden], trainable=True, initializer=tf.contrib.layers.xavier_initializer())\n",
        "  weights['b1'] = tf.Variable(tf.zeros([n_hidden], dtype=tf.float32), trainable=True)\n",
        "                                  \n",
        "  weights['w2'] = tf.transpose(weights['w1'])\n",
        "  weights['b2'] = tf.Variable(tf.zeros([n_inputs], dtype=tf.float32), trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih_PGfUWLF95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_tfrecord_files(path):\n",
        "  files = []\n",
        "  for entry in os.scandir(path):\n",
        "    if entry.is_file() and entry.name.endswith('.tfrecord'):\n",
        "             files.append(entry.path)\n",
        "  return files\n",
        "  \n",
        "  \n",
        "def _parse_example(example):\n",
        "  features = {'height': tf.FixedLenFeature([], tf.int64),\n",
        "              'width': tf.FixedLenFeature([], tf.int64),\n",
        "              'image_raw': tf.FixedLenFeature([], tf.string)}\n",
        "  parsed_features = tf.parse_single_example(example, features)\n",
        "  img = tf.io.decode_raw(parsed_features['image_raw'], tf.uint8)\n",
        "  height = tf.cast(parsed_features['height'], tf.int32)\n",
        "  width = tf.cast(parsed_features['width'], tf.int32)\n",
        "\n",
        "  img_reshaped = tf.manip.reshape(img, [height, width, 3])\n",
        "  imgfl = tf.image.convert_image_dtype(img_reshaped, dtype=tf.float32)\n",
        "  img_norm = tf.expand_dims(imgfl, 0)\n",
        "  img_resized = tf.image.resize_bicubic(img_norm, [target_height, target_width])\n",
        "  img_resized = tf.squeeze(img_resized, 0)\n",
        "  return img_resized\n",
        "\n",
        "\n",
        "def input_function(path, batch_size=1, num_epochs=None, shuffle=False):\n",
        "  tfrecords = get_tfrecord_files(path)\n",
        "  dataset = tf.data.TFRecordDataset(tfrecords)\n",
        "  dataset = dataset.map(_parse_example)\n",
        "  if (shuffle):\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.batch(batch_size).repeat(num_epochs)\n",
        "  iterator = dataset.make_initializable_iterator()\n",
        "  return iterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xd_MYyIBcgcy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"input\")  \n",
        "\n",
        "def create_pca_autoencoder(learning_rate, norm_constant):\n",
        "\n",
        "  norm = tf.constant(norm_constant, tf.float32, name=\"normalization_constant\")\n",
        "  \n",
        "  # input normalization\n",
        "  norm_x = tf.math.xdivy(x, norm)\n",
        "\n",
        "  # input_dims -> n_hidden\n",
        "  multadd1 = tf.add(tf.matmul(norm_x, weights['w1']), weights['b1'])\n",
        "  hidden = transfer_function(multadd1, name=\"output\")\n",
        "    \n",
        "  # reconstruction  \n",
        "  # n_hidden ->  input_dims\n",
        "  y = tf.identity(tf.add(tf.matmul(hidden, weights['w2']), weights['b2']), name=\"y\")\n",
        "      \n",
        "  with tf.variable_scope(\"opt\", reuse=tf.AUTO_REUSE): \n",
        "    cost = tf.reduce_mean(tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=norm_x, logits=y), axis=1, keepdims=True))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate) \n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "      opt = optimizer.minimize(cost)\n",
        "     \n",
        "  return hidden, cost, opt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p8BcbOitLQ5l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_pca_model(training_files_dir,\n",
        "                        validation_files_dir,\n",
        "                        testing_files_dir,\n",
        "                        batch_size, epochs, steps, learning_rate, norm_constant, model_tag):\n",
        "    period_size = 100\n",
        "        \n",
        "    training_iter = input_function(training_files_dir, batch_size)\n",
        "    training_images = training_iter.get_next()\n",
        "    training_features = module(training_images)\n",
        "        \n",
        "    validation_iter = input_function(validation_files_dir, batch_size)\n",
        "    validation_images = validation_iter.get_next()\n",
        "    validation_features = module(validation_images)\n",
        "        \n",
        "    testing_iter = input_function(testing_files_dir, 100)\n",
        "    testing_images = testing_iter.get_next()\n",
        "    testing_features = module(testing_images)\n",
        "        \n",
        "    out1, recon_cost, train_op = create_pca_autoencoder(learning_rate, norm_constant)\n",
        "   \n",
        "    init = tf.global_variables_initializer()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "  \n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    print(\"Train PCA Autoencoder model\")\n",
        "    for i in range(epochs):\n",
        "      sess.run([training_iter.initializer, validation_iter.initializer])\n",
        "      iteration = 0\n",
        "      total_cost = 0.\n",
        "      total_val_cost = 0.\n",
        "      while True:\n",
        "        try:\n",
        "          Xtrain = sess.run(training_features)\n",
        "          train_cost, opt = sess.run([recon_cost, train_op], feed_dict={x: Xtrain})\n",
        "          if (iteration % period_size == 0):\n",
        "            Xvalid = sess.run(validation_features)\n",
        "            validation_cost = sess.run(recon_cost, feed_dict={x: Xvalid})\n",
        "            total_cost += train_cost\n",
        "            total_val_cost += validation_cost\n",
        "          iteration = iteration + 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "          break\n",
        "        if (iteration > steps):\n",
        "          break\n",
        "                \n",
        "      steps_taken = iteration//period_size\n",
        "      avg_train_loss = total_cost/steps_taken\n",
        "      avg_val_loss = total_val_cost/steps_taken\n",
        "      print(\"epoch {0} training cost {1} valid. cost {2}\".format(i+1, avg_train_loss, avg_val_loss))\n",
        "      train_losses.append(avg_train_loss)\n",
        "      valid_losses.append(avg_val_loss)\n",
        "   \n",
        "                         \n",
        "    plt.figure(101)\n",
        "    plt.plot(train_losses)\n",
        "    plt.plot(valid_losses)\n",
        "    plt.plot()\n",
        "    plt.title(\"PCA Autoencoder {0}->{1}\".format(n_inputs, n_hidden))\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"cost\")\n",
        "    plt.legend([\"training\", \"validation\"], loc=\"upper right\")\n",
        "    plt.show()\n",
        "    \n",
        "    w1 = weights['w1'].eval(session=sess)\n",
        "    b1 = weights['b1'].eval(session=sess)\n",
        "    b2 = weights['b2'].eval(session=sess)\n",
        "    \n",
        "    nbins = 64\n",
        "    plt.figure(102)\n",
        "    plt.hist((w1.ravel()), bins=nbins, histtype='bar', stacked=True)\n",
        "    plt.legend(['w1'], loc='upper right')\n",
        "    plt.title('Histogram of Weights')\n",
        "    plt.show()\n",
        "    \n",
        "    plt.figure(103)\n",
        "    plt.hist((b1.ravel(), b2.ravel()), bins=nbins, histtype='bar', stacked=True)\n",
        "    plt.legend(['b1', 'b2'], loc='upper right')\n",
        "    plt.title('Histogram of Biases')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"run test on 100 images\")\n",
        "    sess.run([testing_iter.initializer])\n",
        "    Xtest = sess.run(testing_features)\n",
        "    testing_cost = sess.run([recon_cost], feed_dict={x: Xtest})\n",
        "    print(\"test cost = {0}\".format(testing_cost))\n",
        "    \n",
        "    graphdef = tf.get_default_graph().as_graph_def()\n",
        "    subgraphdef = tf.graph_util.extract_sub_graph(graphdef, ['output'])\n",
        "    subgraphdef = tf.graph_util.remove_training_nodes(subgraphdef, protected_nodes=['output'])\n",
        "    subgraphdef_frozen = tf.graph_util.convert_variables_to_constants(sess, subgraphdef, ['output'])\n",
        "    \n",
        "    print(\"write model: \", frozen_model)\n",
        "    with tf.gfile.GFile(frozen_model, \"wb\") as f:\n",
        "      f.write(subgraphdef_frozen.SerializeToString())\n",
        "        \n",
        "    sess.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Is7JSNCDLWpG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Train autoencoder\")\n",
        "print(\"training files: \", training_files_dir)\n",
        "print(\"validation files: \", validation_files_dir)\n",
        "print(\"testing files: \", testing_files_dir)\n",
        "print(\"learning_rate: \", learning_rate)\n",
        "print(\"batch size: \", batch_size)\n",
        "print(\"epochs: \", epochs)\n",
        "print(\"steps: \", steps)\n",
        "print(\"norm constant: \", norm_constant)\n",
        "\n",
        "train_pca_model(training_files_dir,\n",
        "                    validation_files_dir,\n",
        "                    testing_files_dir,\n",
        "                    batch_size, epochs, steps,\n",
        "                    learning_rate, norm_constant, model_tag)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}