{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_cae_layers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starkdg/pyConvnetPhash/blob/master/train_cae_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bzUdnuEwaAXn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0oNUYgvPa74X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "\n",
        "model_dir = \"/gdrive/My Drive/models\"\n",
        "module_url = \"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\"\n",
        "mobilenetv2_module_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2\"\n",
        "\n",
        "module = hub.Module(mobilenetv2_module_url)\n",
        "target_height, target_width = hub.get_expected_image_size(module)\n",
        "\n",
        "input_dims = 1792\n",
        "\n",
        "training_files_dir = \"/gdrive/My Drive/imageset/train\"\n",
        "validation_files_dir = \"/gdrive/My Drive/imageset/validation\"\n",
        "testing_files_dir = \"/gdrive/My Drive/imageset/test\"\n",
        "\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "steps = 2000\n",
        "alpha = 0.0001\n",
        "lambda_reg = 0.001\n",
        "normalization_constant = 5.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "46CftwY-qFgk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ContractiveAutoencoder(object):\n",
        "  def __init__(self, n_input, n_hidden, lambda_reg = 0.001, optimizer = tf.train.AdamOptimizer(), scope=None):\n",
        "    self.n_input = n_input\n",
        "    self.n_hidden = n_hidden\n",
        "    \n",
        "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
        "      self.weights = self._initialize_weights()\n",
        "      \n",
        "      # model\n",
        "      reg_term = tf.constant(lambda_reg, tf.float32)\n",
        "      self.x = tf.placeholder(tf.float32, [None, n_input])\n",
        "      self.h = tf.nn.sigmoid(tf.add(tf.matmul(self.x,self.weights['w1']), self.weights['b1']), name='hidden')\n",
        "      self.y = tf.identity(tf.add(tf.matmul(self.h, self.weights['w2']), self.weights['b2']), name='reconstructed')\n",
        "    \n",
        "      # Jacobian norm\n",
        "      dhi = tf.square(tf.multiply(self.h, tf.subtract(1., self.h)))                  # N x n_hidden\n",
        "      dwj = tf.reduce_sum(tf.square(self.weights['w2']), axis=1, keepdims=True)      # n_hidden x n_input => n_hidden x 1\n",
        "      jnorm = tf.matmul(dhi, dwj, name=\"jnorm\")                                      # N x 1 \n",
        "      self.jnorm_mean = tf.reduce_mean(jnorm)\n",
        "  \n",
        "      # cost \n",
        "      cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.x, logits=self.y), axis=1, keepdims=True)\n",
        "      self.avg_cost = tf.reduce_mean(tf.add(cost, tf.multiply(reg_term, jnorm)))\n",
        "\n",
        "      with tf.variable_scope('opt', reuse=tf.AUTO_REUSE):\n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(update_ops):\n",
        "          self.train_op = optimizer.minimize(self.avg_cost)\n",
        "     \n",
        "      self.init = tf.global_variables_initializer()\n",
        "      self.sess = tf.Session()\n",
        "      self.sess.run(self.init)\n",
        "      \n",
        "  def _initialize_weights(self):\n",
        "    all_weights = dict()\n",
        "    all_weights['w1'] = tf.get_variable(\"w1\", shape=[self.n_input, self.n_hidden], trainable=True, initializer=tf.contrib.layers.xavier_initializer())\n",
        "    all_weights['b1'] = tf.Variable(tf.zeros([self.n_hidden], dtype = tf.float32), trainable=True)\n",
        "    all_weights['w2'] = tf.transpose(all_weights['w1'], name='w2')\n",
        "    all_weights['b2'] = tf.Variable(tf.zeros([self.n_input], dtype = tf.float32), trainable=True)\n",
        "    return all_weights\n",
        "\n",
        "  def partial_fit(self, X):\n",
        "    avg_cost, jnorm_mean, opt = self.sess.run((self.avg_cost, self.jnorm_mean, self.train_op), feed_dict={self.x: X})\n",
        "    return avg_cost, jnorm_mean\n",
        "\n",
        "  def compute_cost(self, X):\n",
        "    avg_cost, jnorm_mean = self.sess.run((self.avg_cost, self.jnorm_mean), feed_dict={self.x: X})\n",
        "    return avg_cost, jnorm_mean\n",
        "\n",
        "  def transform(self, X):\n",
        "    encoded = self.sess.run(self.h, feed_dict={self.x: X})\n",
        "    return encoded\n",
        "\n",
        "  def reconstruct(self, X):\n",
        "    return self.sess.run(self.y, feed_dict={self.x: X})\n",
        "\n",
        "  def getWeights(self):\n",
        "    return self.sess.run(self.weights['w1'])\n",
        "\n",
        "  def getBiases(self):\n",
        "    return self.sess.run(self.weights['b1'])\n",
        "\n",
        "  def save_weights(self, path, global_step):\n",
        "    saver = tf.train.Saver({'w1': self.weights['w1'],\n",
        "                            'b1': self.weights['b1'],\n",
        "                            'b2': self.weights['b2']}, max_to_keep=2)\n",
        "    saver.save(self.sess, path, global_step=global_step)\n",
        "\n",
        "  def load_weights(self, path):\n",
        "    saver = tf.train.Saver({'w1': self.weights['w1'],\n",
        "                            'b1': self.weights['b1'],\n",
        "                            'b2': self.weights['b2']})\n",
        "    saver.restore(self.sess, path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p9u_e_cTcUiB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_tfrecord_files(path):\n",
        "  files = []\n",
        "  for entry in os.scandir(path):\n",
        "    if entry.is_file() and entry.name.endswith('.tfrecord'):\n",
        "             files.append(entry.path)\n",
        "  return files\n",
        "  \n",
        "  \n",
        "def _parse_example(example):\n",
        "  features = {'height': tf.FixedLenFeature([], tf.int64),\n",
        "              'width': tf.FixedLenFeature([], tf.int64),\n",
        "              'image_raw': tf.FixedLenFeature([], tf.string)}\n",
        "  parsed_features = tf.parse_single_example(example, features)\n",
        "  img = parsed_features['image_raw']\n",
        "  img = tf.io.decode_raw(parsed_features['image_raw'], tf.uint8)\n",
        "  height = tf.cast(parsed_features['height'], tf.int32)\n",
        "  width = tf.cast(parsed_features['width'], tf.int32)\n",
        "  img_reshaped = tf.manip.reshape(img, [height, width, 3])\n",
        "  imgfl = tf.image.convert_image_dtype(img_reshaped, dtype=tf.float32)\n",
        "  imgfl = tf.expand_dims(imgfl, 0)\n",
        "  img_resized = tf.image.resize_bicubic(imgfl, [target_height, target_width])\n",
        "  img_resized = tf.squeeze(img_resized, 0)\n",
        "\n",
        "  return img_resized\n",
        "\n",
        "\n",
        "def input_function(path, batch_size=1, num_epochs=None, shuffle=False):\n",
        "  tfrecords = get_tfrecord_files(path)\n",
        "  dataset = tf.data.TFRecordDataset(tfrecords)\n",
        "  dataset = dataset.map(_parse_example)\n",
        "  if (shuffle):\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.batch(batch_size).repeat(num_epochs)\n",
        "  iterator = dataset.make_initializable_iterator()\n",
        "  return iterator\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "  norm_x = (x - np.amin(x)) / (np.amax(x) - np.amin(x))\n",
        "  return norm_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P9nLfG7jhhhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_autoenc_model(training_files_dir,\n",
        "                        validation_files_dir,\n",
        "                        testing_files_dir,\n",
        "                        batch_size, epochs, steps, alpha,\n",
        "                        lambda_reg):\n",
        "  period_size = 100\n",
        "  training_iter = input_function(training_files_dir, batch_size)\n",
        "  training_images = training_iter.get_next()\n",
        "  training_features = module(training_images)\n",
        "    \n",
        "  validation_iter = input_function(validation_files_dir, batch_size)\n",
        "  validation_images = validation_iter.get_next()\n",
        "  validation_features = module(validation_images)\n",
        "    \n",
        "  testing_iter = input_function(testing_files_dir, 100)\n",
        "  testing_images = testing_iter.get_next()\n",
        "  testing_features = module(testing_images)\n",
        "     \n",
        "  opt = tf.train.AdamOptimizer(learning_rate=alpha) \n",
        "  \n",
        "  cae_n = 3              # layer number to train\n",
        "  saved_n = 1            # save iteration (global step  in saved dir)\n",
        "  dim_from = 512         # from dimension\n",
        "  dim_to = 256           # to dimension\n",
        "  \n",
        "  # uncomment each layer to train each layer \n",
        "  cae1 = ContractiveAutoencoder(input_dims, 1024, lambda_reg=lambda_reg, optimizer=opt, scope='cae1')\n",
        "  cae2 = ContractiveAutoencoder(1024, 512, lambda_reg=lambda_reg, optimizer=opt, scope='cae2')\n",
        "  cae3 = ContractiveAutoencoder(512, 256, lambda_reg=lambda_reg, optimizer=opt, scope='cae3')\n",
        "  \n",
        "  # set to last layer \n",
        "  cae = cae3\n",
        "  \n",
        "  save_dir_str = '/gdrive/My Drive/models/cae{0}/cae.chp-{1}'\n",
        "  save_dir1 =  save_dir_str.format(1, saved_n)\n",
        "  save_dir2 =  save_dir_str.format(2, saved_n)\n",
        "  save_dir3 =  save_dir_str.format(3, saved_n)\n",
        "  save_cae_to_dir = \"/gdrive/My Drive/models/cae{0}/cae.chp\".format(cae_n)\n",
        "  training_cae_title = \"CAE-{0}to{1}\".format(dim_from, dim_to)\n",
        "  \n",
        "  try:\n",
        "    # load all except last layer\n",
        "    cae1.load_weights(save_dir1)\n",
        "    print(\"load weights for cae-1\")\n",
        "    cae2.load_weights(save_dir2)\n",
        "    print(\"load weights for cae-2\")\n",
        "    cae3.load_weights(save_dir3)\n",
        "    print(\"load weights for cae-3\")\n",
        "  except ValueError:\n",
        "    print(\"Train CAE layer \", cae_n)\n",
        "          \n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  fig_n = 1\n",
        "  train_recon_losses = []\n",
        "  valid_recon_losses = []\n",
        "  valid_jnorm_losses = []\n",
        "    \n",
        "  for i in range(epochs):\n",
        "    sess.run([training_iter.initializer, validation_iter.initializer])\n",
        "    iteration = 0\n",
        "    total_train_recon_cost = 0\n",
        "    total_valid_recon_cost = 0\n",
        "    total_train_jnorm_cost = 0\n",
        "    total_valid_jnorm_cost = 0\n",
        "        \n",
        "    while True:\n",
        "      try:\n",
        "        Xtrain = sess.run(training_features)\n",
        "        Xtrain = normalize(Xtrain)\n",
        "        Xtrain = cae1.transform(Xtrain)\n",
        "        Xtrain = cae2.transform(Xtrain)\n",
        "        # last layer to be trained\n",
        "        train_cost, train_jnorm = cae.partial_fit(Xtrain)      \n",
        "        if (iteration % period_size == 0):\n",
        "          Xvalid = sess.run(validation_features)\n",
        "          Xvalid = normalize(Xvalid)\n",
        "          Xvalid = cae1.transform(Xvalid)\n",
        "          Xvalid = cae2.transform(Xvalid)\n",
        "          # check validation cost for last layer\n",
        "          valid_cost, valid_jnorm = cae.compute_cost(Xvalid)\n",
        "          total_train_recon_cost += train_cost\n",
        "          total_valid_recon_cost += valid_cost\n",
        "          total_train_jnorm_cost += train_jnorm\n",
        "          total_valid_jnorm_cost += valid_jnorm\n",
        "  \n",
        "        iteration = iteration + 1\n",
        "      except tf.errors.OutOfRangeError:\n",
        "        break\n",
        "      if (iteration > steps):\n",
        "        break\n",
        "                \n",
        "    \n",
        "    steps_taken = iteration//period_size\n",
        "    avg_train_recon_loss = total_train_recon_cost/steps_taken\n",
        "    avg_valid_recon_loss = total_valid_recon_cost/steps_taken\n",
        "    avg_valid_jnorm_loss = total_valid_jnorm_cost/steps_taken\n",
        "    \n",
        "    print(\"epoch {0} training cost = {1} valid. cost= {2} (jnorm = {3})\".format(\n",
        "       i+1, avg_train_recon_loss, avg_valid_recon_loss, avg_valid_jnorm_loss))\n",
        "    \n",
        "             \n",
        "    train_recon_losses.append(avg_train_recon_loss)\n",
        "    valid_recon_losses.append(avg_valid_recon_loss)\n",
        "    valid_jnorm_losses.append(avg_valid_jnorm_loss)\n",
        "\n",
        "\n",
        "  cae.save_weights(save_cae_to_dir, global_step=saved_n)\n",
        "  plt.figure(fig_n)\n",
        "  plt.plot(train_recon_losses)\n",
        "  plt.plot(valid_recon_losses)\n",
        "  plt.plot(valid_jnorm_losses)\n",
        "  \n",
        "  plt.title(\"{0} Autoencoder Training Losses\".format(training_cae_title))\n",
        "  plt.legend(['train loss', 'valid. loss', 'jnorm loss'], loc='upper right')\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"cost\")\n",
        "  plt.show()\n",
        "\n",
        "  print(\"run test on 100 images\")\n",
        "  sess.run([testing_iter.initializer])\n",
        "  Xtest = sess.run(testing_features)\n",
        "  Xtest = normalize(Xtest)\n",
        "  Xtest = cae1.transform(Xtest)\n",
        "  Xtest = cae2.transform(Xtest)\n",
        "  # compute cost for test set\n",
        "  test_cost, test_jnorm = cae.compute_cost(Xtest)\n",
        "  print(\"test cost = {0}, jnorm = {1}\".format(test_cost, test_jnorm))\n",
        "  sess.close()\n",
        "\n",
        "  fig_n += 1\n",
        "    \n",
        "  w1 = cae.getWeights()\n",
        "  b1 = cae.getBiases()\n",
        "  plt.figure(fig_n)\n",
        "  plt.hist((w1.ravel()), bins=64, histtype='bar', stacked=True)\n",
        "  plt.legend(['w1'], loc='upper right')\n",
        "  plt.title('histogram of weights for last layer')\n",
        "\n",
        "  print(\"w1:\")\n",
        "  print(\"mean: \", np.mean(w1))\n",
        "  print(\"min: \", np.amin(w1))\n",
        "  print(\"max: \", np.amax(w1))\n",
        "  print(\"std: \", np.std(w1))\n",
        "    \n",
        "  fig_n += 1\n",
        "  plt.figure(fig_n)\n",
        "  plt.hist((b1.ravel()), bins=100, histtype='bar', stacked=True)\n",
        "  plt.legend(['b1'], loc='upper right')\n",
        "  plt.title('histogram of biases for last layer')\n",
        "  \n",
        "  print(\"b1:\")\n",
        "  print(\"mean: \", np.mean(b1))\n",
        "  print(\"min: \", np.amin(b1))\n",
        "  print(\"max: \", np.amax(b1))\n",
        "  print(\"std: \", np.std(b1))\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fUTf-Rfaqj_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Train autoencoder\")\n",
        "print(\"training files: \", training_files_dir)\n",
        "print(\"validation files: \", validation_files_dir)\n",
        "print(\"testing files: \", testing_files_dir)\n",
        "print(\"alpha: \", alpha)\n",
        "print(\"lambda reg: \", lambda_reg)\n",
        "print(\"batch size: \", batch_size)\n",
        "print(\"epochs: \", epochs)\n",
        "print(\"steps: \", steps)\n",
        "\n",
        "train_autoenc_model(training_files_dir,\n",
        "                    validation_files_dir,\n",
        "                    testing_files_dir,\n",
        "                    batch_size, epochs, steps,\n",
        "                    alpha, lambda_reg)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}