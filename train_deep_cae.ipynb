{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_deep_cae.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starkdg/pyConvnetPhash/blob/master/train_deep_cae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jjcK55t-TFMq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "n_inputs = 1792\n",
        "jnorm_reg = 0.0\n",
        "\n",
        "model_tag = 0\n",
        "frozen_model = \"/gdrive/My Drive/models/deepautoencoder/mobilenetv2_deep_autoenc_frozen_model{0}.pb\".format(model_tag)\n",
        "\n",
        "training_files_dir = \"/gdrive/My Drive/imageset/train\"\n",
        "validation_files_dir = \"/gdrive/My Drive/imageset/validation\"\n",
        "testing_files_dir = \"/gdrive/My Drive/imageset/test\"\n",
        "\n",
        "batch_size = 10\n",
        "epochs = 15\n",
        "steps = 2000\n",
        "learning_rate = 0.0004\n",
        "\n",
        "model_dir = \"/gdrive/My Drive/models\"\n",
        "module_inception_url = \"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\"\n",
        "module_mobilenetv2_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2\"\n",
        "\n",
        "\n",
        "# module graph and session (for extracting features from hub module)\n",
        "module_graph = tf.Graph()\n",
        "module_sess = tf.Session(graph=module_graph)  \n",
        "with module_graph.as_default():\n",
        "  module = hub.Module(module_mobilenetv2_url)\n",
        "  target_height, target_width = hub.get_expected_image_size(module)\n",
        "\n",
        "# autoencoder graph  and session\n",
        "aec_graph = tf.Graph()\n",
        "aec_sess = tf.Session(graph=aec_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OzKxh7rdTjgJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_weights():\n",
        "  weights = dict()\n",
        "  with tf.variable_scope(\"weights\", reuse=tf.AUTO_REUSE):\n",
        "    weights['w1'] = tf.get_variable('w1', shape=[n_inputs, 1024], trainable=True)\n",
        "    weights['b1'] = tf.get_variable('b1', shape=(1024), trainable=True)\n",
        "\n",
        "    weights['w2'] = tf.get_variable('w2', shape=[1024, 512], trainable=True)\n",
        "    weights['b2'] = tf.get_variable('b2', shape=(512), trainable=True)\n",
        "  \n",
        "    weights['w3'] = tf.get_variable('w3', shape=[512, 256], trainable=True)\n",
        "    weights['b3'] = tf.get_variable('b3', shape=(256), trainable=True)\n",
        "    \n",
        "    weights['w4'] = tf.transpose(weights['w3'])\n",
        "    weights['b4'] = tf.get_variable('b4', shape=(512), trainable=True)\n",
        "  \n",
        "    weights['w5'] = tf.transpose(weights['w2'])\n",
        "    weights['b5'] = tf.get_variable('b5', shape=(1024), trainable=True)\n",
        "  \n",
        "    weights['w6'] = tf.transpose(weights['w1'])\n",
        "    weights['b6'] = tf.get_variable('b6', shape=(n_inputs), trainable=True)\n",
        "  \n",
        "  return weights     \n",
        "\n",
        "  \n",
        "def load_weights(**wts):  \n",
        "  saver1 = tf.train.Saver({'w1': wts['w1'], \n",
        "                           'b1': wts['b1'],\n",
        "                           'b2': wts['b6']})\n",
        "  try:\n",
        "    saver1.restore(aec_sess, '/gdrive/My Drive/models/cae1/cae.chp-1')\n",
        "    print(\"restore layers 1, 6\")\n",
        "  except:\n",
        "    print(\"unable to restore layers 1, 6\")\n",
        "    \n",
        "    \n",
        "  saver2 = tf.train.Saver({'w1': wts['w2'],\n",
        "                           'b1': wts['b2'],\n",
        "                           'b2': wts['b5']})\n",
        "  try:\n",
        "    saver2.restore(aec_sess, '/gdrive/My Drive/models/cae2/cae.chp-1')\n",
        "    print(\"restore layers 2, 5\")\n",
        "  except:\n",
        "    print(\"unable to restore layers 2, 5\")\n",
        "  \n",
        "  saver3 = tf.train.Saver({'w1': wts['w3'],\n",
        "                           'b1': wts['b3'],\n",
        "                           'b2': wts['b4']})\n",
        "  try:\n",
        "    saver3.restore(aec_sess,'/gdrive/My Drive/models/cae3/cae.chp-1')\n",
        "    print(\"restore layers 3, 4\")\n",
        "  except:\n",
        "    print(\"unable to restore layers 3, 4\")\n",
        "   \n",
        " \n",
        "def plot_weights_and_biases(**weights):\n",
        "  with aec_sess.as_default():\n",
        "    w1 = weights['w1'].eval()\n",
        "    w2 = weights['w2'].eval()\n",
        "    w3 = weights['w3'].eval()\n",
        "    b1 = weights['b1'].eval()\n",
        "    b2 = weights['b2'].eval()\n",
        "    b3 = weights['b3'].eval()\n",
        "    b4 = weights['b4'].eval()\n",
        "    b5 = weights['b5'].eval()\n",
        "    b6 = weights['b6'].eval()\n",
        "    \n",
        "  plt.figure(102)\n",
        "  plt.hist([w1.ravel(), w2.ravel(), w3.ravel()], bins=100, histtype='bar', stacked=True)\n",
        "  plt.legend(['w1', 'w2', 'w3'], loc='upper right')\n",
        "  plt.title('Histogram of weights')\n",
        "  plt.show()\n",
        "    \n",
        "  plt.figure(103)\n",
        "  plt.hist([b1.ravel(), b2.ravel(), b3.ravel(), b4.ravel(), b5.ravel(), b6.ravel()], bins=100, histtype='bar', stacked=True)\n",
        "  plt.legend(['b1', 'b2', 'b3', 'b4', 'b5', 'b6'], loc='upper right')\n",
        "  plt.title('Histogram of biases')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xd_MYyIBcgcy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_deep_autoencoder(learning_rate, lambda_reg):\n",
        "    \n",
        "  with aec_graph.as_default():\n",
        "    weights = get_weights()\n",
        "    x = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"input\")  \n",
        "    \n",
        "    reg_term = tf.constant(lambda_reg, tf.float32, name=\"jnorm_reg\")\n",
        "    \n",
        "    num_x = tf.subtract(x, tf.reduce_min(x))\n",
        "    den_x = tf.subtract(tf.reduce_max(x), tf.reduce_min(x))\n",
        "    norm_x = tf.math.xdivy(num_x, den_x, name=\"normalization\")\n",
        "    \n",
        "    # input_dims -> 1024\n",
        "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(norm_x, weights['w1']), weights['b1']), name=\"output1024\")\n",
        "    \n",
        "    # 1024 -> 512\n",
        "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weights['w2']), weights['b2']), name=\"output512\")\n",
        "    \n",
        "    # 512 -> 256\n",
        "    layer3 = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weights['w3']), weights['b3']), name=\"output256\")\n",
        "    \n",
        "    # reconstruction  \n",
        "    # 256 -> 512\n",
        "    layer4 = tf.identity(tf.add(tf.matmul(layer3, weights['w4']), weights['b4']), name=\"layer4\")\n",
        "    \n",
        "    # 512 -> 1024\n",
        "    layer5 = tf.identity(tf.add(tf.matmul(layer4, weights['w5']), weights['b5']), name=\"layer5\") \n",
        "    \n",
        "    # 1024 -> input_dims\n",
        "    y = tf.identity(tf.add(tf.matmul(layer5, weights['w6']), weights['b6']), name=\"y\")\n",
        "    \n",
        "    # Jacobian norm\n",
        "    dhi1 = tf.square(tf.multiply(layer1, tf.subtract(1., layer1)))                  # N x 1024\n",
        "    dwj1 = tf.reduce_sum(tf.square(weights['w6']), axis=1, keepdims=True)           # 1024 x n_input => 1024 x 1\n",
        "    jnorm1 = tf.matmul(dhi1, dwj1, name=\"jnorm1\")                                     # N x 1 \n",
        "      \n",
        "    # Jacobian norm\n",
        "    dhi2 = tf.square(tf.multiply(layer2, tf.subtract(1., layer2)))                  # N x 512\n",
        "    dwj2 = tf.reduce_sum(tf.square(weights['w5']), axis=1, keepdims=True)           # 512 x n_input => 512 x 1\n",
        "    jnorm2 = tf.matmul(dhi2, dwj2, name=\"jnorm2\")                                   # N x 1 \n",
        "        \n",
        "    # Jacobian norm\n",
        "    dhi3 = tf.square(tf.multiply(layer3, tf.subtract(1., layer3)))                  # N x 256\n",
        "    dwj3 = tf.reduce_sum(tf.square(weights['w4']), axis=1, keepdims=True)           # 256 x 1792 => 256 x 1\n",
        "    jnorm3 = tf.matmul(dhi3, dwj3, name=\"jnorm3\")                                   # N x 1 \n",
        "        \n",
        "    jnorm_total = tf.add_n([jnorm1, jnorm2, jnorm3])\n",
        "    avg_jnorm = tf.reduce_mean(jnorm_total)\n",
        "    cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=norm_x, logits=y), axis=1, keepdims=True)\n",
        "    avg_cost = tf.reduce_mean(tf.add(cost, tf.multiply(reg_term, jnorm_total)))\n",
        "\n",
        "    with tf.variable_scope(\"opt\", reuse=tf.AUTO_REUSE): \n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate) \n",
        "      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "      with tf.control_dependencies(update_ops):\n",
        "        train_op = optimizer.minimize(cost)\n",
        "     \n",
        "    return x, layer3, weights, avg_cost, avg_jnorm, train_op\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih_PGfUWLF95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_tfrecord_files(path):\n",
        "  files = []\n",
        "  for entry in os.scandir(path):\n",
        "    if entry.is_file() and entry.name.endswith('.tfrecord'):\n",
        "             files.append(entry.path)\n",
        "  return files\n",
        "  \n",
        "  \n",
        "def _parse_example(example):\n",
        "  features = {'height': tf.FixedLenFeature([], tf.int64),\n",
        "              'width': tf.FixedLenFeature([], tf.int64),\n",
        "              'image_raw': tf.FixedLenFeature([], tf.string)}\n",
        "  parsed_features = tf.parse_single_example(example, features)\n",
        "  img = tf.io.decode_raw(parsed_features['image_raw'], tf.uint8)\n",
        "  height = tf.cast(parsed_features['height'], tf.int32)\n",
        "  width = tf.cast(parsed_features['width'], tf.int32)\n",
        "\n",
        "  img_reshaped = tf.manip.reshape(img, [height, width, 3])\n",
        "  imgfl = tf.image.convert_image_dtype(img_reshaped, dtype=tf.float32)\n",
        "  img_norm = tf.expand_dims(imgfl, 0)\n",
        "  img_resized = tf.image.resize_bicubic(img_norm, [target_height, target_width])\n",
        "  img_resized = tf.squeeze(img_resized, 0)\n",
        "  return img_resized\n",
        "\n",
        "\n",
        "def input_function(path, batch_size=1, num_epochs=None, shuffle=False):\n",
        "  tfrecords = get_tfrecord_files(path)\n",
        "  dataset = tf.data.TFRecordDataset(tfrecords)\n",
        "  dataset = dataset.map(_parse_example)\n",
        "  if (shuffle):\n",
        "    dataset = dataset.shuffle(10000)\n",
        "  dataset = dataset.batch(batch_size).repeat(num_epochs)\n",
        "  iterator = dataset.make_initializable_iterator()\n",
        "  return iterator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p8BcbOitLQ5l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_autoenc_model(training_files_dir,\n",
        "                        validation_files_dir,\n",
        "                        testing_files_dir,\n",
        "                        batch_size, epochs, steps, learning_rate, lambda_reg):\n",
        "    period_size = 100\n",
        "    input_dims = n_inputs\n",
        "    \n",
        "    x, y, wts, recon_cost, jnorm_cost, train_op = create_deep_autoencoder(learning_rate, lambda_reg)\n",
        "    \n",
        "    with module_graph.as_default():  \n",
        "      training_iter = input_function(training_files_dir, batch_size)\n",
        "      training_images = training_iter.get_next()\n",
        "      training_features = module(training_images)\n",
        "        \n",
        "      validation_iter = input_function(validation_files_dir, batch_size)\n",
        "      validation_images = validation_iter.get_next()\n",
        "      validation_features = module(validation_images)\n",
        "        \n",
        "      testing_iter = input_function(testing_files_dir, 100)\n",
        "      testing_images = testing_iter.get_next()\n",
        "      testing_features = module(testing_images)\n",
        "       \n",
        "      module_init = tf.global_variables_initializer()  \n",
        "    \n",
        "    module_sess.run(module_init)\n",
        "  \n",
        "    with aec_graph.as_default():\n",
        "      aec_init = tf.initializers.global_variables()\n",
        "      \n",
        "    aec_sess.run(aec_init)  \n",
        "    \n",
        "    load_weights(**wts)\n",
        "    \n",
        "    print(\"plot weights and biases before fine-tuning\")\n",
        "    plot_weights_and_biases(**wts)\n",
        "    \n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    jnorm_losses = []\n",
        "    print(\"Train Deep Autoencoder\")\n",
        "    for i in range(epochs):\n",
        "      module_sess.run([training_iter.initializer, validation_iter.initializer])\n",
        "      iteration = 0\n",
        "      total_cost = 0.\n",
        "      total_val_cost = 0.\n",
        "      total_val_jnorm_cost = 0.\n",
        "      while True:\n",
        "        try:\n",
        "          Xtrain = module_sess.run(training_features)\n",
        "          train_cost, opt = aec_sess.run([recon_cost, train_op], feed_dict={x: Xtrain})\n",
        "          if (iteration % period_size == 0):\n",
        "            Xvalid = module_sess.run(validation_features)\n",
        "            validation_cost, valid_jnorm_cost = aec_sess.run([recon_cost, jnorm_cost], feed_dict={x: Xvalid})\n",
        "            total_cost += train_cost\n",
        "            total_val_cost += validation_cost\n",
        "            total_val_jnorm_cost += valid_jnorm_cost\n",
        "          iteration = iteration + 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "          break\n",
        "        if (iteration > steps):\n",
        "          break\n",
        "                \n",
        "      steps_taken = iteration//period_size\n",
        "      avg_train_loss = total_cost/steps_taken\n",
        "      avg_val_loss = total_val_cost/steps_taken\n",
        "      avg_jnorm_loss = total_val_jnorm_cost/steps_taken\n",
        "      print(\"epoch {0} training cost {1} valid. cost {2} (jnorm {3})\".format(i+1, avg_train_loss, avg_val_loss, avg_jnorm_loss))\n",
        "      train_losses.append(avg_train_loss)\n",
        "      valid_losses.append(avg_val_loss)\n",
        "      jnorm_losses.append(avg_jnorm_loss)\n",
        "      \n",
        "    plt.figure(101)\n",
        "    plt.plot(train_losses)\n",
        "    plt.plot(valid_losses)\n",
        "    plt.plot(jnorm_losses)\n",
        "    plt.plot()\n",
        "    plt.title(\"Deep Autoencoder 1792->1024->512->256\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"cost\")\n",
        "    plt.legend([\"training\", \"validation\", \"jnorm\"], loc=\"upper right\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"run test on 100 images\")\n",
        "    module_sess.run([testing_iter.initializer])\n",
        "    Xtest = module_sess.run(testing_features)\n",
        "    testing_cost, testing_jnorm = aec_sess.run([recon_cost, jnorm_cost], feed_dict={x: Xtest})\n",
        "    print(\"test cost = {0} (jnorm = {1})\".format(testing_cost, testing_jnorm))\n",
        "\n",
        "    print(\"plot weights and biases after fine-tuning\")\n",
        "    plot_weights_and_biases(**wts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6fwcXUO43-W-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_aec_graph_to_file():\n",
        "  aec_graphdef = aec_graph.as_graph_def()\n",
        "  aec_subgraphdef = tf.compat.v1.graph_util.extract_sub_graph(aec_graphdef, ['output256'])\n",
        "  aec_subgraphdef = tf.compat.v1.graph_util.remove_training_nodes(aec_subgraphdef)\n",
        "  aec_subgraphdef_frozen = tf.compat.v1.graph_util.convert_variables_to_constants(aec_sess, aec_subgraphdef, ['output256'])\n",
        "        \n",
        "  with tf.gfile.GFile(frozen_model, \"wb\") as f:\n",
        "    f.write(aec_subgraphdef_frozen.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Is7JSNCDLWpG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Train autoencoder\")\n",
        "print(\"training files: \", training_files_dir)\n",
        "print(\"validation files: \", validation_files_dir)\n",
        "print(\"testing files: \", testing_files_dir)\n",
        "print(\"learning_rate: \", learning_rate)\n",
        "print(\"batch size: \", batch_size)\n",
        "print(\"epochs: \", epochs)\n",
        "print(\"steps: \", steps)\n",
        "print(\"jnorm reg: \", jnorm_reg)\n",
        "train_autoenc_model(training_files_dir,\n",
        "                    validation_files_dir,\n",
        "                    testing_files_dir,\n",
        "                    batch_size, epochs, steps,\n",
        "                    learning_rate, jnorm_reg)\n",
        "\n",
        "print(\"save autoencoder graph to file: \", frozen_model)\n",
        "save_aec_graph_to_file()\n",
        "  \n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}